---
# tasks file for ollama

- name: Install dependencies for Ollama
  ansible.builtin.apt:
    name:
      - curl
      - systemd
    state: present
    update_cache: true
  tags:
    - ollama
    - packages
    - ai
    - llm
    - dependencies

- name: Check if Ollama is already installed
  ansible.builtin.command: ollama --version
  register: ollama_installed
  changed_when: false
  failed_when: false
  tags:
    - ollama
    - ai
    - llm

- name: Display current Ollama version if installed
  ansible.builtin.debug:
    msg: "Ollama already installed: {{ ollama_installed.stdout }}"
  when: ollama_installed.rc == 0
  tags:
    - ollama
    - ai
    - llm

- name: Download Ollama installation script
  ansible.builtin.get_url:
    url: https://ollama.com/install.sh
    dest: /tmp/ollama_install.sh
    mode: '0755'
  when: ollama_installed.rc != 0
  tags:
    - ollama
    - packages
    - ai
    - llm

- name: Install Ollama
  ansible.builtin.shell: |
    set -o pipefail
    /tmp/ollama_install.sh
  args:
    executable: /bin/bash
    creates: /usr/local/bin/ollama
  when: ollama_installed.rc != 0
  tags:
    - ollama
    - packages
    - ai
    - llm

- name: Clean up installation script
  ansible.builtin.file:
    path: /tmp/ollama_install.sh
    state: absent
  when: ollama_installed.rc != 0
  tags:
    - ollama
    - packages
    - ai
    - llm
    - cleanup

- name: Create Ollama systemd override directory
  ansible.builtin.file:
    path: /etc/systemd/system/ollama.service.d
    state: directory
    mode: '0755'
    owner: root
    group: root
  tags:
    - ollama
    - ai
    - llm
    - service
    - configuration

- name: Configure Ollama to listen on all interfaces
  ansible.builtin.copy:
    dest: /etc/systemd/system/ollama.service.d/override.conf
    mode: '0644'
    owner: root
    group: root
    content: |
      [Service]
      Environment="OLLAMA_HOST={{ ollama_host }}"
  notify: Restart ollama service
  tags:
    - ollama
    - ai
    - llm
    - service
    - configuration

- name: Reload systemd daemon
  ansible.builtin.systemd:
    daemon_reload: true
  tags:
    - ollama
    - ai
    - llm
    - service

- name: Ensure Ollama service is enabled and started
  ansible.builtin.systemd:
    name: ollama
    state: "{{ ollama_service_state }}"
    enabled: "{{ ollama_service_enabled }}"
  tags:
    - ollama
    - ai
    - llm
    - service

- name: Wait for Ollama service to be ready
  ansible.builtin.wait_for:
    host: localhost
    port: 11434
    delay: 2
    timeout: 30
  tags:
    - ollama
    - ai
    - llm
    - service

- name: Check if models are already pulled
  ansible.builtin.command: ollama list
  register: ollama_list
  changed_when: false
  failed_when: false
  tags:
    - ollama
    - ai
    - llm
    - models

- name: Pull Ollama models
  ansible.builtin.command: "ollama pull {{ item }}"
  loop: "{{ ollama_models }}"
  when: item not in ollama_list.stdout
  register: ollama_pull_result
  changed_when: "'success' in ollama_pull_result.stdout or ollama_pull_result.rc == 0"
  tags:
    - ollama
    - ai
    - llm
    - models

- name: Verify Ollama installation
  ansible.builtin.command: ollama --version
  register: ollama_version_output
  changed_when: false
  tags:
    - ollama
    - ai
    - llm

- name: List installed Ollama models
  ansible.builtin.command: ollama list
  register: ollama_models_list
  changed_when: false
  tags:
    - ollama
    - ai
    - llm
    - models

- name: Pull Open WebUI Docker image
  community.docker.docker_image:
    name: "{{ open_webui_image }}"
    source: pull
    state: present
  when: open_webui_enabled
  tags:
    - ollama
    - ai
    - llm
    - docker
    - open-webui

- name: Manage Open WebUI Docker container
  community.docker.docker_container:
    name: "{{ open_webui_container_name }}"
    image: "{{ open_webui_image }}"
    state: started
    restart_policy: unless-stopped
    ports:
      - "{{ open_webui_port }}:{{ open_webui_internal_port }}"
    env:
      OLLAMA_API_BASE_URL: "{{ ollama_api_base_url }}"
    etc_hosts:
      host.docker.internal: host-gateway
    comparisons:
      image: strict
      env: strict
      ports: strict
  when: open_webui_enabled
  tags:
    - ollama
    - ai
    - llm
    - docker
    - open-webui

- name: Wait for Open WebUI to be ready
  ansible.builtin.wait_for:
    host: localhost
    port: "{{ open_webui_port }}"
    delay: 5
    timeout: 60
  when: open_webui_enabled
  tags:
    - ollama
    - ai
    - llm
    - docker
    - open-webui

- name: Display Ollama installation info
  ansible.builtin.debug:
    msg: |
      Ollama has been installed successfully!

      Version: {{ ollama_version_output.stdout }}
      Listening on: {{ ollama_host }}

      Installed models:
      {{ ollama_models_list.stdout }}

      Open WebUI:
      {% if open_webui_enabled %}
      - Web interface: http://localhost:{{ open_webui_port }}
      - Container: {{ open_webui_container_name }}
      - Status: docker ps -f name={{ open_webui_container_name }}
      - Logs: docker logs {{ open_webui_container_name }}
      {% else %}
      - Open WebUI is disabled
      {% endif %}

      Useful commands:
      - ollama --version       : Show Ollama version
      - ollama list            : List installed models
      - ollama pull <model>    : Download a model
      - ollama run <model>     : Run a model interactively
      - ollama serve           : Start Ollama service
      - ollama ps              : List running models
      - ollama rm <model>      : Remove a model

      Getting started:
      1. Access Open WebUI at http://localhost:{{ open_webui_port }}
         - Create an account (first user becomes admin)
         - Select the qwen2.5-coder model from the dropdown
         - Start chatting!

      2. Run a model interactively via CLI:
         ollama run qwen2.5-coder:latest

      3. Use Ollama with API (service running on {{ ollama_host }}):
         curl http://localhost:11434/api/generate -d '{
           "model": "qwen2.5-coder:latest",
           "prompt": "Write a hello world function in Python"
         }'

      4. Pull additional models:
         ollama pull llama3.2
         ollama pull codellama
         ollama pull mistral
         ollama pull deepseek-coder

      Available popular models:
      - qwen2.5-coder       : Optimized for code generation
      - codellama           : Meta's code-specialized model
      - deepseek-coder      : DeepSeek's code model
      - llama3.2            : Latest Llama model
      - mistral             : Mistral AI's model
      - phi3                : Microsoft's efficient model

      Configuration:
      - Ollama service: systemctl status ollama
      - Ollama host: {{ ollama_host }}
      - Data directory: ~/.ollama
      - API endpoint: http://{{ ollama_host }}
      - Open WebUI port: {{ open_webui_port }}

      Docker Management:
      - View logs: docker logs -f {{ open_webui_container_name }}
      - Restart: docker restart {{ open_webui_container_name }}
      - Stop: docker stop {{ open_webui_container_name }}
      - Start: docker start {{ open_webui_container_name }}

      Tips:
      - Use 'ollama run <model>' for interactive chat
      - Press Ctrl+D to exit interactive mode
      - Models are cached locally after first pull
      - Check available models at https://ollama.com/library
      - Open WebUI provides a user-friendly interface for Ollama
      - Access Open WebUI from any device on your network at http://<your-ip>:{{ open_webui_port }}

      Documentation:
      - Ollama: https://ollama.com
      - GitHub: https://github.com/ollama/ollama
      - Model library: https://ollama.com/library
      - API docs: https://github.com/ollama/ollama/blob/main/docs/api.md
      - Open WebUI: https://github.com/open-webui/open-webui
  tags:
    - ollama
    - ai
    - llm
